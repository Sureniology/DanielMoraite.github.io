{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import larq as lq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "# Normalize pixel values to be between -1 and 1\n",
    "train_images, test_images = train_images / 127.5 - 1, test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(input_quantizer=\"ste_sign\", kernel_quantizer=\"ste_sign\", kernel_constraint=\"weight_clip\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(lq.layers.QuantConv2D(32, (3, 3), kernel_quantizer=\"ste_sign\", kernel_constraint=\"weight_clip\", use_bias=False, input_shape=(28, 28, 1)))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "\n",
    "model.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\n",
    "model.add(tf.keras.layers.BatchNormalization(scale=False))\n",
    "model.add(tf.keras.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer                  Outputs             # 1-bit    # 32-bit\n",
      "---------------------  ----------------  ---------  ----------\n",
      "quant_conv2d           (-1, 26, 26, 32)        288           0\n",
      "max_pooling2d          (-1, 13, 13, 32)          0           0\n",
      "batch_normalization    (-1, 13, 13, 32)          0          96\n",
      "quant_conv2d_1         (-1, 11, 11, 64)      18432           0\n",
      "max_pooling2d_1        (-1, 5, 5, 64)            0           0\n",
      "batch_normalization_1  (-1, 5, 5, 64)            0         192\n",
      "quant_conv2d_2         (-1, 3, 3, 64)        36864           0\n",
      "batch_normalization_2  (-1, 3, 3, 64)            0         192\n",
      "flatten                (-1, 576)                 0           0\n",
      "quant_dense            (-1, 64)              36864           0\n",
      "batch_normalization_3  (-1, 64)                  0         192\n",
      "quant_dense_1          (-1, 10)                640           0\n",
      "batch_normalization_4  (-1, 10)                  0          30\n",
      "activation             (-1, 10)                  0           0\n",
      "Total                                        93088         702\n",
      "\n",
      "Total params: 93790\n",
      "Trainable params: 93322\n",
      "Non-trainable params: 468\n"
     ]
    }
   ],
   "source": [
    "lq.models.summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 91s 2ms/step - loss: 0.6450 - acc: 0.9082\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.4708 - acc: 0.9626\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 77s 1ms/step - loss: 0.4449 - acc: 0.9693\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 88s 1ms/step - loss: 0.4326 - acc: 0.9735\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.4272 - acc: 0.9758\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.4243 - acc: 0.9771\n",
      "10000/10000 [==============================] - 3s 344us/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, batch_size=64, epochs=6)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 93.30 %\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test accuracy {test_acc * 100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
